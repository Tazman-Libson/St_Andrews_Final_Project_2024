---
title: Pseudoresiduals of Multivariate Gaussians
author:
  # see ?rjournal_article for more information
  - name: Author One
    affiliation: Affiliation
    address:
    - line 1
    - line 2
    url: https://journal.r-project.org
    orcid: 0000-0002-9079-593X
    email:  author1@work
  - name: Author Two
    url: https://journal.r-project.org
    email: author2@work
    orcid: 0000-0002-9079-593X
    affiliation: Affiliation 1
    address:
    - line 1 affiliation 1
    - line 2 affiliation 1
    affiliation2: Affiliation 2
    address2:
    - line 1 affiliation 2
    - line 2 affiliation 2
  - name: Author Three
    url: https://journal.r-project.org
    email: author3@work
    affiliation: Affiliation
    address:
    - line 1 affiliation
    - line 2 affiliation
abstract: >
  

preamble: |
  % Any extra LaTeX you need in the preamble
  
bibliography: RJreferences.bib
output: rticles::rjournal_article
editor_options: 
  markdown: 
    wrap: 72
---
```{r, includ = F}
library(readr)
library(tidyverse)
library(matlib)
library(mvtnorm)


Apple <- read_csv("GitHub/St_Andrews_Final_Project_2024/Apple.csv")
Microsoft <- read_csv("GitHub/St_Andrews_Final_Project_2024/Microsoft.csv")
Intel <- read_csv("GitHub/St_Andrews_Final_Project_2024/Intel.csv")
Meta <- read_csv("GitHub/St_Andrews_Final_Project_2024/META.csv")

price_to_returns<- function(Stock){
  Stock$last <- as.numeric(sub('.', '', Stock$`Close/Last`))
  returns_name <- paste(deparse(substitute(Stock)), ' Returns')
  output <- Stock %>% 
    #Get rid of earliest date
    filter(Date != Stock$Date[length(Stock$Date)]) %>%
    #Add the offset list of prices
    mutate(day_before = Stock$last[-1])%>%
    #take log of ratio of price and price of previous day
    mutate(Returns = 100*(log(last) - log(day_before)))%>%
    select(Date, Returns)
  return(output)
}


apl <- price_to_returns(Apple)
mic <- price_to_returns(Microsoft)
tel <- price_to_returns(Intel)
met <- price_to_returns(Meta)

example_returns <- apl %>%
  left_join(mic, join_by(Date)) %>%
  left_join(tel, join_by(Date)) %>%
  left_join(met, join_by(Date)) %>%
  rename(Apple = Returns.x, Microsoft = Returns.y, Intel = Returns.x.x, Meta = Returns.y.y )

#Function to easily create TPM matrices
stan_starting_tpm <- function(probs){
  m <- length(probs)
  probs_diag <- diag(probs)
  remain <- (rep(1,m) - probs)/(m-1)
  out <- (probs_diag - diag(remain) ) + remain
  return(out)
}


#Need to make a function that takes the VCV array and turns it into the relevant parameters:
#ar is a VCV as described above
mvn.ar_to_vec <- function(ar){
  #Sub function in order to use sapply will take the square matrix in each index of VCV and
  #return a vector of the upper triangular elements
  symmat_to_vec <- function(s){
    return(ar[,,s][upper.tri(ar[,,s], diag = T)])
  }
  m <- dim(ar)[3] #number of states
  t <- sapply(1:m, symmat_to_vec)
  as.vector(t)
}


#Function to turn vector into array:
mvn.vec_to_ar <- function(vector, n, m){
  vecs <- matrix(vector, ncol = (n*n -(n*(n-1)/2)), nrow = m, byrow = T)
  fun <- function(s){
    return(as.vector(symMat(vecs[s,], byrow = T)))
  }
  dat <- sapply(1:m, FUN = fun)
  dat <- as.vector(dat)
  dat <- array(data = dat, dim = c(n,n,m))
  return(dat)
}







#In Order to use nlm, need a function that doesn't have restrictions on it's parameters
#The means aren't restricted
#Variance and covariance matrix is non-negative and symmetric. 
#TPM row values must sum to 1 and are also non-negative. Means that tpm has only m(m-1) parameters to estimate

mvn.n2w <- function(mod, stationary){
  #Reparameterization for tpm:
  tpm <- mod$TPM
  m <- dim(tpm)[1]
  tpm <- log(tpm/diag(tpm))
  tpm <- as.vector(tpm[!diag(m)])
  #Initial Distribution:
  if(stationary == F){
    id <- mod$ID
    id<-log(id[-1]/id[1])
  }
  #var/covariances:
  vcv <-mod$VCV
  n <- dim(vcv)[2]
  vcv <- mvn.ar_to_vec(vcv) #turn ar into vector
  vcv <- log(vcv) #take log
  #means (don't need reparam, just vectorisation)
  mns <- as.vector(mod$MEANS)
  params <- c(tpm, vcv, mns)
  if(!stationary){
    params <- c(params, id)
  }
  return(params)
}
#function for fining stationary distribution from a given tpm
stat_dist <- function(tpm){
  m <- dim(tpm)[1]
  delta<-solve(t(diag(m)-tpm+1),rep(1,m))
  return(delta)
}

mvn.w2n <- function(params, m, n, stationary){
  #index 3 is start of tpm, which goes for 3 + m*(m-1)
  tpm_last <- (m*(m-1))
  vcv_last <- tpm_last + m*n*n - m*n*(n-1)/2
  mns_last <- vcv_last + m*n
  tpm <- params[1:tpm_last]
  TPM <- diag(m)
  TPM[!TPM] <- exp(tpm)
  TPM <- TPM/apply(TPM,1,sum)
  vcv <- params[(tpm_last+1):vcv_last]
  vcv <- exp(vcv)
  VCV <- mvn.vec_to_ar(vcv, n, m)
  means <- params[(vcv_last+1):mns_last]
  MEANS <- matrix(means, nrow = m, ncol = n, byrow = T)
  if(stationary){
    ID <- stat_dist(TPM)
  }else{
    id <- tail(params, n = (m-1))
    foo<-c(1,exp(id))
    ID<-foo/sum(foo)
  }
  return(
    list(
      MEANS = MEANS,
      VCV = VCV,
      TPM = TPM,
      ID = ID
    )
  )
}




#Function for state_dependent distribution probability matrix
#Inputs:
#mod <- list as described above
#X <- vector of legnth n with the observations at a single time
#Outputs:
#an m x m diagnonal matrix with the marginal probability for each state-dependent distribtion
mvn.p_matrix <- function(mod, X){
  mvn <- function(m){
    sig <- mod$VCV[,,m]
    means <- mod$MEANS[m,]
    return(dmvnorm(X, mean = means, sigma = sig, checkSymmetry = F))
  }
  m <- dim(mod$TPM)[1]
  probs <- lapply(1:m, mvn)
  return(diag(probs))
}

mvn.HMM_mllk <- function(parvec, X, m, n, stationary){
  mod <- mvn.w2n(parvec, m, n, stationary)
  print(mod)
  tpm <- mod$TPM
  t <- dim(X)[1] # number of observations
  phi <- mod$ID
  l <- 0 #log likelihood
  for(i in 1:t){
    v <- phi %*% tpm %*% mvn.p_matrix(mod, X[i,])
    forw[i,] <- v
    u <- sum(v)
    l <- l + log(u)
    phi <- v/u #rescaled vector of forward probabilities
  }
  return(-l)
}
nlm_mod
mvn.HMM_mllk(mvn.n2w(nlm_mod, T), tmatrix, 3, 4, T)



#Checking if aT *1 is equal to the likelihood
log(exp(Lt)%*%c(1,1,1)) 
mvn.HMM_ml_mod_fit <- function(mod, data, stationary = T){
  n <- dim(mod$VCV)[2] # number of variables for multivariate norm
  m <- dim(mod$TPM)[1] # number of states
  parvec <- mvn.n2w(mod, stationary)
  print(parvec)
  print( mvn.w2n(parvec, m, n, stationary))
  fit <- nlm(mvn.HMM_mllk, parvec, X = data, n= n, m= m, stationary = stationary, steptol = 1e-5)
  print(fit)
  return(mvn.w2n(fit$estimate, m = m, n = n, stationary = stationary))
}

#Testing:
ret_matrix <- matrix(
  data = c(example_returns$Apple, example_returns$Microsoft, example_returns$Intel, example_returns$Meta),
  ncol = 4
)
#Give some more thought 
#LKJ Distribution

vcv_vec <- c(
  as.vector(diag(4)+0.1),
  as.vector(diag(4)*2+0.2),
  as.vector(diag(4)*3+0.3)
)

returns_tmod <- list(
  TPM = stan_starting_tpm(c(.9,.8,.7)),
  ID = NA,
  MEANS = matrix(0, nrow = 3, ncol = 4),
  VCV = array(data = vcv_vec, dim = c(4,4,3)),
  Stationary = TRUE
)

tmatrix <- ret_matrix[1:100,] #Smaller Matrix for small testing

mvnlktest <- mvn.HMM_ml_mod_fit(returns_tmod, tmatrix)
nlm_mod <- mvnlktest

mvn.lforward<-function(x, mod){
  m <- dim(mod$TPM)[1]
  lenx <- dim(x)[1] #number of observations
  lalpha        <- matrix(NA,ncol = m, nrow = lenx)
  foo           <- mod$ID %*% mvn.p_matrix(mod, X = x[1,])
  sumfoo        <- sum(foo)
  lscale        <- log(sumfoo)
  foo           <- foo/sumfoo
  lalpha[1,]    <- lscale+log(foo)
  for (i in 2:(lenx))
  {
    foo          <- foo%*%mod$TPM %*% mvn.p_matrix(mod, X = x[i,])
    sumfoo       <- sum(foo)
    lscale       <- lscale+log(sumfoo)
    foo          <- foo/sumfoo
    lalpha[i,]   <- log(foo)+lscale
  }
  return(lalpha)
}

Lt <- mvn.lforward(mod = nlm_mod, x = tmatrix)
#Checking if aT *1 is equal to the likelihood
log(exp(Lt)%*%c(1,1,1)) 
#It is, we're good
#Computing log backward probabilities
mvn.lbackward<-function(x,mod)
{
  lenx          <- dim(x)[1]
  m          <- dim(mod$TPM)[1]
  lbeta      <- matrix(NA,ncol = m,nrow = lenx)
  lbeta[lenx,]  <- rep(0,m)
  foo        <- rep(1/m,m)
  lscale     <- log(m)
  for (i in (lenx-1):1)
  {
    foo        <- mod$TPM%*%mvn.p_matrix(mod, x[i+1,])%*%foo
    lbeta[i,]  <- log(foo)+lscale
    sumfoo     <- sum(foo)
    foo        <- foo/sumfoo
    lscale     <- lscale+log(sumfoo)
  }
  return(lbeta)
}
#Testing Backward Probabilities: 
log(exp(log(exp(mvn.lbackward(tmatrix, nlm_mod))*exp(mvn.lforward(mod = nlm_mod, x = tmatrix))))%*%c(1,1,1)) 

mc1step <- function(x, tpm){
  n <- dim(tpm)[1]
  row <- tpm[x,]
  #print(row)
  rand <- runif(1)
  threshold <- 0
  for(index in 1:n){
    threshold <- threshold + row[index]
    #print(threshold)
    if(rand < threshold){
      #print(rand)
      return(index)
    }
  }
}

initial_state <- function(delta, tpm){
  n <- dim(tpm)[1]
  rand <- runif(1)
  threshold <- 0
  for(index in 1:n){
    threshold <- threshold + delta[index]
    #print(threshold)
    if(rand < threshold){
      #print(rand)
      return(index)
    }
  }
}

mvn.generate_single_obs <- function(state, mod){
  vcv <- mod$VCV[,,state]
  #print(vcv)
  means <- mod$MEANS[state,]
  #print(means)
  obs <- rmvnorm(1, mean = means, sigma = vcv)
  return(as.vector(obs))
}


mvn.generate_sample <- function(nobs, mod, seed = 138140){
  set.seed(seed)
  m <- dim(mod$TPM)[1]
  n <- dim(mod$VCV)[2]
  state <- initial_state(mod$ID, mod$TPM)
  sample <- matrix(nrow = nobs,  ncol = n)
  for(i in 1:nobs){
    sample[i,] <- mvn.generate_single_obs(state, mod)
    state <- mc1step(state, mod$TPM)
  }
  return(sample)
}

mvn.cdf <- function(x,mod){
  lenx         <- dim(x)[1]
  m         <- dim(mod$TPM)[1]
  dxc       <- matrix(NA,nrow=lenx,ncol=1)
  Px        <- matrix(NA,nrow=lenx,ncol=m)
  for (j in 1:lenx){ Px[j,] <- mvn.cumul_vec(mod, x[j,])}
  #print(Px)
  la        <- mvn.lforward(x,mod)
  lb        <- mvn.lbackward(x,mod)
  la        <- rbind(log(mod$ID),la)
  lafact    <- apply(la,1,max)
  lbfact    <- apply(lb,1,max)
  for (i in 1:lenx)
  {
    foo      <- (exp(la[i,]-lafact[i])%*%mod$TPM)*exp(lb[i,]-lbfact[i])
    foo      <- foo/sum(foo)
    #if(i ==1){print(foo)}
    dxc[i]  <- sum(Px[i,]%*%t(foo))
  }
  return(dxc)
}


#Testing if Pseudoresiduals make sense:

nlm_gen_sample <- mvn.generate_sample(200, nlm_mod)

#Fit Model to Generated Data:

gen_mod <- mvn.HMM_ml_mod_fit(returns_tmod, nlm_gen_sample, T)



```
## Introduction
Hidden Markov Models are a very versatile generative way to model
discrete time/space data. One of the many ways they can be used is
modelling returns in finance. One of the main state dependant
distributions for these purposes is the multivariate gaussian
distribution. This allows different stocks to have an impact on
each other's variance, which is what one would naturally expect for
stocks in related fields. This paper has code for fitting multivariate
gaussian HMMs to financial data. This is taken from the Zucchini et al
(2016) textbook for fitting models using direct liklihood maximization.
The code for fitting a model is then verified by generating data from a
fitted model, then fitting another model to the generated dataset. For
further model verification, normal pseudoresiduals, as described by
Zucchini et al are used. There have been some issues which will be
described later.
## HMM basics

An m-state HMM is made up of 3 components, an m x m transition probability matrix $\Gamma$ (TPM), an initial state distribution $\delta$, and state dependent distributions. In a financial context, the relevant data is a time series taken at equally spaced intervals. Each time is considered to have a state, and for each state there is a state-dependent distribution from which the observed quantity comes. It is assumed there is no way to observe the states, hence the name Hidden Markov.

The likelihood of a set of observations for a given model can be caluclated through recursive matrix multiplation as described by Zucchini and MacDonald.

\${L_T=\delta P(x_1) \Gamma P(x_2) ... \Gamma P(x_T)1'} \$

This equation will be used for model fitting.

## Model Fitting

### Method

Model fitting is done through the `nlm` function. In order to use this, the parameters need to be unrestricted. This is done through different transformations. The required parameters are the transition probability matrix, the initial distribution, and the means and covariance matrix for each state dependent multivariate Gaussian. No transformation is needed for the means, since they are already unrestricted real values. The variance covariance matrix needs to be symmetric so only the upper triangular matrix (including the main diagonal of variances) needs to be estimated. The covariance matrix values are also non-negative. To map them onto the real numbers, the log is taken to map it to the real numbers to produce the working parameter. The transformations for the TPM and initial distributions are both identical to methods laid out by Zucchini and MacDonald.

In order to avoid underflow, the process of standardizing the probabilities and summing the log of the standardization factors is used, again as described by Zucchini and MacDonald.

###Results

## Pseudoresiduals

Pseudoresiduals are a method of checking a model. If the observations are transformed by the cumulative distribution determined by the fitted model, if the model is valid one would expect these to be Uniform Distributed. The cumulative distribution function is the conditional density function of an observation conditioned on every other observation.

\$P(X_t=x \| X^{-t}=x^{-t}) = \frac{\alpha_{t-1} \Gamma P(x) \beta_t}{\alpha_{t-1} \Gamma \beta_t} 1' \$ (Formula for Conditional Cumulative Distribution Function)

For the purpose of outlier detection, one can then take the inverse standard normal function, `qnorm`. These values are called normal pseudoresiduals. This method is laid out by Zucchini and MacDonald.

This is where problems start to arise. Essentially, for both the model fitted to the real world data, and the model fitted to the generated data, the pseudoresiduals are roughly \$ \~ N(-1, 1)\$ distributed, instead of $N(0,1)$ distributed.

```{r}
#gen_mod is model from generated data
#nlm_mod is model fitted to real world data

#mvn.cdf gives cdf values for all observered values

#histogram and qqnorm of nlm_mod:
nlm_resids <- mvn.cdf(tmatrix, nlm_mod) #tmatrix is matrix of returns for 100 days
hist(qnorm(nlm_resids),main = 'Histogram of Normal Pseudoresids of nlm_mod')
print("Variance:")
print(var(qnorm(nlm_resids)))
```

```{r}
qqnorm(qnorm(nlm_resids))
qqline(qnorm(nlm_resids))
```

```{r}

gen_resids <- mvn.cdf(nlm_gen_sample, gen_mod)
hist(qnorm(gen_resids),main = 'Histogram of Normal Pseudoresids of gen_mod')

print("Variance:")
print(var(qnorm(gen_resids)))
```

```{r}
qqnorm(qnorm(gen_resids))
qqline(qnorm(gen_resids))
```
